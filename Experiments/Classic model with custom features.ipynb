{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMAKzzgHRXIAT2bYveWSeQi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbqK47gMIgm6","executionInfo":{"status":"ok","timestamp":1667402224506,"user_tz":-60,"elapsed":16803,"user":{"displayName":"SARVAJITH G","userId":"12955820379118045214"}},"outputId":"cdd2e69f-2553-4f68-da5c-09ac887fe729"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import argparse\n","import pandas as pd \n","import numpy as np\n","import time as t\n","import re\n","from nltk.stem import PorterStemmer\n","from sklearn.utils import resample, shuffle\n","pd.set_option('mode.chained_assignment', None)\n","\n","import scipy.sparse as sp\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.feature_extraction import DictVectorizer\n","\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","ps = PorterStemmer()"],"metadata":{"id":"6IIzlnhVJaQV","executionInfo":{"status":"ok","timestamp":1667402235914,"user_tz":-60,"elapsed":9787,"user":{"displayName":"SARVAJITH G","userId":"12955820379118045214"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","test_df = pd.read_csv('/content/gdrive/MyDrive/Data/test.tsv',sep = \"\\t\", header = 0)"],"metadata":{"id":"MPOZY-K3J73H","executionInfo":{"status":"ok","timestamp":1667402237864,"user_tz":-60,"elapsed":615,"user":{"displayName":"SARVAJITH G","userId":"12955820379118045214"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(\"Dataset size:\", len(test_df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667402238950,"user_tz":-60,"elapsed":3,"user":{"displayName":"SARVAJITH G","userId":"12955820379118045214"}},"outputId":"5679b587-3258-4437-ed22-3831a501a332","id":"vUJR0mGaMVpb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset size: 859\n"]}]},{"cell_type":"code","source":["\n","test_df.columns = ['tweet', 'task']"],"metadata":{"id":"_sdjPwR1RK5J","executionInfo":{"status":"ok","timestamp":1667402241681,"user_tz":-60,"elapsed":268,"user":{"displayName":"SARVAJITH G","userId":"12955820379118045214"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["test_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"qd_36RJoMfNK","executionInfo":{"status":"ok","timestamp":1666868875609,"user_tz":-120,"elapsed":380,"user":{"displayName":"SARVAJITH G","userId":"12955820379118045214"}},"outputId":"30271e85-22ec-44f3-b755-6bb8cca35515"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               tweet task\n","0  #ConstitutionDay is revered by Conservatives, ...  NOT\n","1  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...  NOT\n","2  #Watching #Boomer getting the news that she is...  NOT\n","3  #NoPasaran: Unity demo to oppose the far-right...  OFF\n","4           . . . What the fuck did he do this time?  OFF"],"text/html":["\n","  <div id=\"df-1c8b845d-a2fc-430e-9731-65d1124b7224\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>task</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>#Watching #Boomer getting the news that she is...</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n","      <td>OFF</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>. . . What the fuck did he do this time?</td>\n","      <td>OFF</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c8b845d-a2fc-430e-9731-65d1124b7224')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1c8b845d-a2fc-430e-9731-65d1124b7224 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1c8b845d-a2fc-430e-9731-65d1124b7224');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["def apply_stem(tweet):\n","\twords = [ps.stem(w) for w in tweet.split()]\n","\treturn ' '.join(words)\n","\n","def remove_newline(tweet):\n","\t_tweet = re.sub('\\n', '', tweet)\n","\treturn _tweet\n","\n","\t\n","def apply_lemma(tweet):\n","\tdoc = nlp(tweet)\n","\tlemma = [token.lemma_ for token in doc]\n","\treturn ' '.join(lemma)\n"," \n","def remove_stopwords(tweet):\n","\tdoc = nlp(tweet)\n","\tstop = [token.text for token in doc if not token.is_stop and not token.is_punct]\n","\treturn ' '.join(stop)\n"," \t\n","\n","def clean_data(tweet):\n","    splitted_tweet = tweet.lower().split()\n","    clean_tweet = []\n","    previous_word = None\n","    user_count = 0\n","    for word in splitted_tweet:\n","        #if word not in spacy_stopwords:\n","        word = re.sub(\"[#@]\",\"\",word)\n","        word = re.sub(\"!\",\" !\",word)\n","        word = re.sub(\"[?]\",\" ?\",word)\n","        \n","        if(word == \"user\"):\n","          user_count += 1\n","          \n","        if(word == \"user\" and previous_word == \"user\"):\n","          pass\n","        else:\n","          clean_tweet.append(word)\n","          \n","        previous_word = word\n","          \n","    return \" \".join(clean_tweet), user_count\n","\n","# Calculating number of Global Positioning Entity in a text\n","def count_gpe(txt):\n","\treturn sum([1 for token in nlp(txt).ents if token.label_ == 'GPE'])\n","\n","\n","# Claculating Number of Organisation in a Text\n","def count_org(txt):\n","\treturn sum([1 for token in nlp(txt).ents if token.label_ == 'ORG'])\n","\n","\n","# Calculating Number of Sentence in a text\n","def count_sentence(txt):\n","\tdoc = nlp(txt)\n","\treturn len([sent.text for sent in doc.sents])\n","\n","\n","#Extract only Noun and Proper Noun\n","def extract_noun(tweet):\n","\tdoc = nlp(tweet)\n","\tcleaned_doc = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and (token.pos_ == 'NOUN' or token.pos_ == 'PROPN')]\n","\treturn ' '.join(cleaned_doc)\n","\n","\n","# Adding Pos Tag with the corresponding words\n","def spacy_pos(tweet):\n","\tdoc = nlp(tweet)\n","\tcleaned = [token.lemma_ + '_' + token.pos_ for token in doc if not token.is_stop and not token.is_punct]\n","\treturn ' '.join(cleaned)\n","\n","\n","#Normalize the Custom Features\n","def normalize(df):\n","\tdf['sentence_count'] /= df['sentence_count'].max()\n","\tdf['gpe_count'] /= df['gpe_count'].max()\n","\tdf['org_count'] /= df['org_count'].max()\n","\t\n","\treturn df\n"],"metadata":{"id":"xpUego5rY7-T","executionInfo":{"status":"ok","timestamp":1667402245202,"user_tz":-60,"elapsed":251,"user":{"displayName":"SARVAJITH G","userId":"12955820379118045214"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","  test_df = test_df.merge(test_df.tweet.apply(lambda x:pd.Series({'preprocessed':clean_data(x)[0], 'user_count': clean_data(x)[1]})), left_index=True, right_index=True)\n","  test_df.drop('user_count', inplace=True, axis=1)\n","  start = t.time()\n","  test_df['gpe_count'] = [sum([1 for token in nlp(txt).ents if token.label_ == 'GPE']) for txt in test_df['tweet']]\n","  stop = t.time()\n","  print(\"\\n Count GPE Time for test set: {}\".format(stop - start))\n","  start = t.time()\n","  test_df['org_count'] = [sum([1 for token in nlp(txt).ents if token.label_ == 'ORG']) for txt in test_df['tweet']]\n","  stop = t.time()\n","  print(\"\\n Count Name_entity Time for test set: {}\".format(stop - start))\n","  start = t.time()\n","  test_df['sentence_count'] = [len([sent.text for sent in nlp(tweet).sents]) for tweet in test_df['tweet']]\n","  stop = t.time()\n","  print(\"\\n Count Sentence for test set: {}\".format(stop - start))\n","  start = t.time()\n","  test_df['pos_tagged'] = [' '.join([token.lemma_ + '_' + token.pos_ for token in nlp(tweet) if not token.is_stop and not token.is_punct])for tweet in test_df['preprocessed']]\n","  stop = t.time()\n","  print(\"\\n Adding Pos Tag to test set: {}\".format(stop - start))\n","  start = t.time()\n","  test_df['noun'] = [' '.join([token.lemma_ for token in nlp(tweet) if not token.is_stop and not token.is_punct and (token.pos_ == 'NOUN' or token.pos_ == 'PROPN')]) for tweet in test_df['preprocessed']]\n","  stop = t.time()\n","  print(\"\\n Noun and Proper Noun Extraction for test set: {}\".format(stop - start))\n","  test_df = normalize(test_df)\n","  test_df.to_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_custom_test.csv', index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqy_RJ6ADXfR","executionInfo":{"status":"ok","timestamp":1666869029822,"user_tz":-120,"elapsed":44433,"user":{"displayName":"SARVAJITH G","userId":"12955820379118045214"}},"outputId":"6e6dab60-3e2c-4df2-8579-54c12f758267"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Count GPE Time for test set: 9.452808856964111\n","\n"," Count Name_entity Time for test set: 9.230441808700562\n","\n"," Count Sentence for test set: 8.813258409500122\n","\n"," Adding Pos Tag to test set: 8.258421659469604\n","\n"," Noun and Proper Noun Extraction for test set: 7.770461559295654\n"]}]},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import svm\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score, classification_report\n","import pandas as pd\n","import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","import spacy\n","from sklearn.preprocessing import LabelBinarizer\n","nlp = spacy.load(\"en_core_web_sm\")\n","import argparse\n","import scipy.sparse as sp\n","import pickle\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"9UVG4WW71Q4F","executionInfo":{"status":"ok","timestamp":1667402255248,"user_tz":-60,"elapsed":1045,"user":{"displayName":"SARVAJITH G","userId":"12955820379118045214"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def identity(x):\n","    \"\"\"Dummy function that just returns the input\"\"\"\n","    return x\n","\n","\n","\n","def tokenizer(tweet):\n","    doc = nlp(tweet)\n","    tokens = [word.text for word in doc]\n","    return tokens\n","\n","\n","def get_score(classifier, X_test, Y_test):\n","    # Given a trained model, predict the label of a new set of data.\n","    Y_pred = classifier.predict(X_test)\n","    # Calculates the accuracy score of the trained model by comparing predicted labels with actual labels.\n","    acc = accuracy_score(Y_test, Y_pred)\n","    test_preds = pd.DataFrame(Y_pred).to_csv('/content/gdrive/MyDrive/Data/output2.csv')\n","    print(classification_report(Y_test, Y_pred))\n","\n","    return acc\n","\n","\n","def base_model(vec, X_train, Y_train):\n","    print(\"Navie Bayes Classification\")\n","    model = MultinomialNB()\n","\n","    classifier = Pipeline([('vec', vec), ('cls', model)])\n","    classifier.fit(X_train, Y_train)\n","\n","    return classifier\n","\n","\n","def optimize_rf(vec, X_train, Y_train, seed):\n","    print(\"Random Forest Classification\")\n","    model = RandomForestClassifier(criterion='gini', n_estimators=233, max_depth=10, max_features=0.064,\n","                                   random_state=seed)\n","\n","    classifier = Pipeline([('vec', vec), ('cls', model)])\n","    classifier.fit(X_train, Y_train)\n","\n","    return classifier\n","\n","\n","def optimize_knn(vec, X_train, Y_train):\n","    print(\"KNN Classification\")\n","    model = KNeighborsClassifier(n_neighbors=118, weights='uniform', n_jobs=-1)\n","\n","    classifier = Pipeline([('vec', vec), ('cls', model)])\n","    classifier.fit(X_train, Y_train)\n","\n","    return classifier\n","\n","\n","def optimize_dt(vec, X_train, Y_train):\n","    print(\"Decision Tree Classification\")\n","    model = DecisionTreeClassifier(\n","        splitter='best',\n","        max_depth=14,\n","        max_features=0.81,\n","        criterion='entropy',\n","        random_state=0\n","    )\n","\n","    classifier = Pipeline([('vec', vec), ('cls', model)])\n","    classifier.fit(X_train, Y_train)\n","\n","    return classifier\n","\n","\n","def optimize_svm(vec, X_train, Y_train, seed):\n","    print(\"SVM classification\")\n","    if vec is None:\n","        svm_ = svm.SVC(kernel='linear', C=1.14, random_state=seed)\n","    else:\n","        svm_ = Pipeline([('vec', vec), ('cls', svm.SVC(kernel='linear', C=1.14, random_state=seed))])\n","\n","    svm_.fit(X_train, Y_train)\n","\n","    return svm_\n","\n","\n","def custom_feature(row):\n","    dic = {}\n","    dic['org_count'] = row['org_count']\n","    dic['sentence_count'] = row['sentence_count']\n","    dic['gpe_count'] = row['gpe_count']\n","    return dic\n","\n","\n","def ensemble(vec, X_train, Y_train, seed):\n","    print(\"Ensemble of Naive Bayes, Random Forest and SVM\")\n","\n","    nb = Pipeline([('vec_cn', vec), ('cls', MultinomialNB())])\n","    rf = Pipeline([('vec_tf', vec), ('cls', RandomForestClassifier(criterion='gini', n_estimators=233, max_depth=10,\n","                                                                   max_features=0.064, n_jobs=-1, random_state=seed))])\n","    svm_ = Pipeline([('vec_tf', vec), ('cls', svm.SVC(kernel='linear', C=1.14, random_state=seed))])\n","\n","    estimators = [('nb', nb), ('rf', rf), ('svm', svm_)]\n","\n","    ensemble_classifier = VotingClassifier(estimators, voting='hard')\n","    classifier = ensemble_classifier.fit(X_train, Y_train)\n","\n","    return classifier"],"metadata":{"id":"okforpVI1W47","executionInfo":{"status":"ok","timestamp":1667402256986,"user_tz":-60,"elapsed":318,"user":{"displayName":"SARVAJITH G","userId":"12955820379118045214"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","\n","    \"\"\" Below code refactored for the format python LFD_assignment2.py -i <trainset> -ts <testset>.\n","        Normally, it is used with split_data function to experiment with different classifiers. \"\"\"\n","\n","    train = pd.read_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_train.csv')\n","    test= pd.read_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_custom_test.csv')\n","    X_train, Y_train = train['preprocessed'], train['task']\n","\n","    X_test, Y_test = test['preprocessed'], test['task']\n","    # Create custom features dictionary\n","    train_dic = [custom_feature(row) for index, row in train.iterrows()]\n","    test_dic = [custom_feature(row) for index, row in test.iterrows()]\n","\n","    dic_train_matr = DictVectorizer().fit_transform(train_dic)\n","    dic_test_matr = DictVectorizer().fit_transform(test_dic)\n","\n","    # Applying TF-IDF on text\n","    vec = TfidfVectorizer().fit(train['preprocessed'])\n","\n","    train_word_mat = vec.transform(train['preprocessed'])\n","    val_word_mat = vec.transform(test['preprocessed'])\n","\n","    train_mat = sp.hstack((train_word_mat, dic_train_matr), format='csr')\n","    val_mat = sp.hstack((val_word_mat, dic_test_matr), format='csr')\n","\n","    classifier = optimize_svm(None, train_mat, Y_train, 32)\n","\n","    acc = get_score(classifier, val_mat, Y_test)\n","    print(\"\\n Accuracy: {}\".format(acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1VnroXaIxSb","executionInfo":{"status":"ok","timestamp":1667402277473,"user_tz":-60,"elapsed":16626,"user":{"displayName":"SARVAJITH G","userId":"12955820379118045214"}},"outputId":"97943885-4636-4f22-afc8-5949f3d25af1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM classification\n","              precision    recall  f1-score   support\n","\n","         NOT       0.88      0.69      0.77       620\n","         OFF       0.48      0.75      0.59       239\n","\n","    accuracy                           0.71       859\n","   macro avg       0.68      0.72      0.68       859\n","weighted avg       0.77      0.71      0.72       859\n","\n","\n"," Accuracy: 0.7066356228172294\n"]}]}]}