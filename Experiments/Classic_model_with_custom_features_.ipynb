{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbqK47gMIgm6",
        "outputId": "8a1f26c4-a417-4dd6-f4cd-9d0dd3318deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import time as t\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.utils import resample, shuffle\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "\n",
        "import scipy.sparse as sp\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "6IIzlnhVJaQV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_stem(tweet):\n",
        "\twords = [ps.stem(w) for w in tweet.split()]\n",
        "\treturn ' '.join(words)\n",
        "\n",
        "def remove_newline(tweet):\n",
        "\t_tweet = re.sub('\\n', '', tweet)\n",
        "\treturn _tweet\n",
        "\n",
        "\t\n",
        "def apply_lemma(tweet):\n",
        "\tdoc = nlp(tweet)\n",
        "\tlemma = [token.lemma_ for token in doc]\n",
        "\treturn ' '.join(lemma)\n",
        " \n",
        "def remove_stopwords(tweet):\n",
        "\tdoc = nlp(tweet)\n",
        "\tstop = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\treturn ' '.join(stop)\n",
        " \t\n",
        "\n",
        "def clean_data(tweet):\n",
        "    splitted_tweet = tweet.lower().split()\n",
        "    clean_tweet = []\n",
        "    previous_word = None\n",
        "    user_count = 0\n",
        "    for word in splitted_tweet:\n",
        "        #if word not in spacy_stopwords:\n",
        "        word = re.sub(\"[#@]\",\"\",word)\n",
        "        word = re.sub(\"!\",\" !\",word)\n",
        "        word = re.sub(\"[?]\",\" ?\",word)\n",
        "        \n",
        "        if(word == \"user\"):\n",
        "          user_count += 1\n",
        "          \n",
        "        if(word == \"user\" and previous_word == \"user\"):\n",
        "          pass\n",
        "        else:\n",
        "          clean_tweet.append(word)\n",
        "          \n",
        "        previous_word = word\n",
        "          \n",
        "    return \" \".join(clean_tweet), user_count\n",
        "\n",
        "# Calculating number of Global Positioning Entity in a text\n",
        "def count_gpe(txt):\n",
        "\treturn sum([1 for token in nlp(txt).ents if token.label_ == 'GPE'])\n",
        "\n",
        "\n",
        "# Claculating Number of Organisation in a Text\n",
        "def count_org(txt):\n",
        "\treturn sum([1 for token in nlp(txt).ents if token.label_ == 'ORG'])\n",
        "\n",
        "\n",
        "# Calculating Number of Sentence in a text\n",
        "def count_sentence(txt):\n",
        "\tdoc = nlp(txt)\n",
        "\treturn len([sent.text for sent in doc.sents])\n",
        "\n",
        "\n",
        "#Extract only Noun and Proper Noun\n",
        "def extract_noun(tweet):\n",
        "\tdoc = nlp(tweet)\n",
        "\tcleaned_doc = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and (token.pos_ == 'NOUN' or token.pos_ == 'PROPN')]\n",
        "\treturn ' '.join(cleaned_doc)\n",
        "\n",
        "\n",
        "# Adding Pos Tag with the corresponding words\n",
        "def spacy_pos(tweet):\n",
        "\tdoc = nlp(tweet)\n",
        "\tcleaned = [token.lemma_ + '_' + token.pos_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\treturn ' '.join(cleaned)\n",
        "\n",
        "\n",
        "#Normalize the Custom Features\n",
        "def normalize(df):\n",
        "\tdf['sentence_count'] /= df['sentence_count'].max()\n",
        "\tdf['gpe_count'] /= df['gpe_count'].max()\n",
        "\tdf['org_count'] /= df['org_count'].max()\n",
        "\t\n",
        "\treturn df\n"
      ],
      "metadata": {
        "id": "xpUego5rY7-T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  test_df = test_df.merge(test_df.tweet.apply(lambda x:pd.Series({'preprocessed':clean_data(x)[0], 'user_count': clean_data(x)[1]})), left_index=True, right_index=True)\n",
        "  test_df.drop('user_count', inplace=True, axis=1)\n",
        "  start = t.time()\n",
        "  test_df['gpe_count'] = [sum([1 for token in nlp(txt).ents if token.label_ == 'GPE']) for txt in test_df['tweet']]\n",
        "  stop = t.time()\n",
        "  print(\"\\n Count GPE Time for test set: {}\".format(stop - start))\n",
        "  start = t.time()\n",
        "  test_df['org_count'] = [sum([1 for token in nlp(txt).ents if token.label_ == 'ORG']) for txt in test_df['tweet']]\n",
        "  stop = t.time()\n",
        "  print(\"\\n Count Name_entity Time for test set: {}\".format(stop - start))\n",
        "  start = t.time()\n",
        "  test_df['sentence_count'] = [len([sent.text for sent in nlp(tweet).sents]) for tweet in test_df['tweet']]\n",
        "  stop = t.time()\n",
        "  print(\"\\n Count Sentence for test set: {}\".format(stop - start))\n",
        "  start = t.time()\n",
        "  test_df['pos_tagged'] = [' '.join([token.lemma_ + '_' + token.pos_ for token in nlp(tweet) if not token.is_stop and not token.is_punct])for tweet in test_df['preprocessed']]\n",
        "  stop = t.time()\n",
        "  print(\"\\n Adding Pos Tag to test set: {}\".format(stop - start))\n",
        "  start = t.time()\n",
        "  test_df['noun'] = [' '.join([token.lemma_ for token in nlp(tweet) if not token.is_stop and not token.is_punct and (token.pos_ == 'NOUN' or token.pos_ == 'PROPN')]) for tweet in test_df['preprocessed']]\n",
        "  stop = t.time()\n",
        "  print(\"\\n Noun and Proper Noun Extraction for test set: {}\".format(stop - start))\n",
        "  test_df = normalize(test_df)\n",
        "  test_df.to_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_custom_test.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqy_RJ6ADXfR",
        "outputId": "d5c67249-a848-410e-d55d-6ee104222b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Count GPE Time for test set: 11.76517391204834\n",
            "\n",
            " Count Name_entity Time for test set: 8.520888805389404\n",
            "\n",
            " Count Sentence for test set: 8.366979837417603\n",
            "\n",
            " Adding Pos Tag to test set: 8.46169900894165\n",
            "\n",
            " Noun and Proper Noun Extraction for test set: 7.821171998977661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import spacy\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import argparse\n",
        "import scipy.sparse as sp\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "9UVG4WW71Q4F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity(x):\n",
        "    \"\"\"Dummy function that just returns the input\"\"\"\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_score(classifier, X_test, Y_test,output_file):\n",
        "    # Given a trained model, predict the label of a new set of data.\n",
        "    Y_pred = classifier.predict(X_test)\n",
        "    # Calculates the accuracy score of the trained model by comparing predicted labels with actual labels.\n",
        "    acc = accuracy_score(Y_test, Y_pred)\n",
        "    if output_file:\n",
        "      test_preds = pd.DataFrame(Y_pred).to_csv('/content/gdrive/MyDrive/Data/output_custfeatures.csv')\n",
        "    print(classification_report(Y_test, Y_pred))\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def optimize_svm(vec, X_train, Y_train, seed):\n",
        "    print(\"SVM classification\")\n",
        "    if vec is None:\n",
        "        svm_ = svm.SVC(kernel='linear', C=1.14, random_state=seed)\n",
        "    else:\n",
        "        svm_ = Pipeline([('vec', vec), ('cls', svm.SVC(kernel='linear', C=1.14, random_state=seed))])\n",
        "\n",
        "    svm_.fit(X_train, Y_train)\n",
        "\n",
        "    return svm_\n",
        "\n",
        "\n",
        "def custom_feature(row):\n",
        "    dic = {}\n",
        "    dic['org_count'] = row['org_count']\n",
        "    dic['sentence_count'] = row['sentence_count']\n",
        "    dic['gpe_count'] = row['gpe_count']\n",
        "    return dic\n"
      ],
      "metadata": {
        "id": "okforpVI1W47"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    output_file = False\n",
        "    Tfidf = False\n",
        "\n",
        "    train = pd.read_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_train.csv')\n",
        "    test= pd.read_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_custom_test.csv')\n",
        "    \n",
        "    X_train, Y_train = train['preprocessed'], train['task']\n",
        "\n",
        "    X_test, Y_test = test['preprocessed'], test['task']\n",
        "    # Create custom features dictionary\n",
        "    train_dic = [custom_feature(row) for index, row in train.iterrows()]\n",
        "    test_dic = [custom_feature(row) for index, row in test.iterrows()]\n",
        "\n",
        "    dic_train_matr = DictVectorizer().fit_transform(train_dic)\n",
        "    dic_test_matr = DictVectorizer().fit_transform(test_dic)\n",
        "\n",
        "    # Applying TF-IDF on text\n",
        "    if Tfidf:\n",
        "      vec = TfidfVectorizer().fit(train['preprocessed'])\n",
        "    else:\n",
        "      vec = CountVectorizer().fit(train['preprocessed'])\n",
        "\n",
        "    train_word_mat = vec.transform(train['preprocessed'])\n",
        "    val_word_mat = vec.transform(test['preprocessed'])\n",
        "\n",
        "    train_mat = sp.hstack((train_word_mat, dic_train_matr), format='csr')\n",
        "    val_mat = sp.hstack((val_word_mat, dic_test_matr), format='csr')\n",
        "\n",
        "    classifier = optimize_svm(None, train_mat, Y_train, 32)\n",
        "\n",
        "    acc = get_score(classifier, val_mat, Y_test,output_file)\n",
        "    print(\"\\n Accuracy: {}\".format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1VnroXaIxSb",
        "outputId": "1b128efe-a78a-4a07-f51d-637381dfd7a1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NOT       0.82      0.69      0.75       647\n",
            "         OFF       0.56      0.71      0.62       352\n",
            "\n",
            "    accuracy                           0.70       999\n",
            "   macro avg       0.69      0.70      0.69       999\n",
            "weighted avg       0.72      0.70      0.70       999\n",
            "\n",
            "\n",
            " Accuracy: 0.6976976976976977\n"
          ]
        }
      ]
    }
  ]
}