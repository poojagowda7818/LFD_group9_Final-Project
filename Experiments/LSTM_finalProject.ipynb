{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fm75QTbTvkxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37a0acd3-7a8c-40d1-9339-e43aad1533c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZkK9yH9wtsBx"
      },
      "outputs": [],
      "source": [
        "import random as python_random\n",
        "import json\n",
        "import argparse\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Embedding, LSTM, Input, Dropout, BatchNormalization, concatenate, SpatialDropout1D, Reshape\n",
        "from keras.layers import Dropout, Conv1D, MaxPooling1D, LSTM,Concatenate, Dense, GlobalMaxPooling1D,GlobalAveragePooling1D, Lambda, Bidirectional, GRU \n",
        "from keras.initializers import Constant\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from gensim.models import FastText\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Make reproducible as much as possible\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "python_random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3DcC3Ef65rwR",
        "outputId": "4e191c50-3cb9-4ea7-c843-5b35e84a23cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "G-3uD3oAuKq9"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Read the glove embeddings\n",
        "def read_embeddings(glove_vec):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
        "        for line in f:\n",
        "          values = line.split();\n",
        "          word = values[0]\n",
        "          coefs = np.asarray(values[1:], dtype='float32')\n",
        "          embeddings_index[word] = coefs\n",
        "\n",
        "    print(\"Found %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "    return embeddings_index\n",
        "\n",
        "\n",
        "#Create embedding layer\n",
        "def get_emb_matrix(emb,voc, maxLen,dim):\n",
        "    '''Get embedding matrix given vocab and the embeddings'''\n",
        "    num_tokens = len(voc)+1\n",
        "    # Prepare embedding matrix to the correct size\n",
        "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "    for word, i in voc.items():\n",
        "      embedding_vector = emb.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    embedding_layer = Embedding(input_dim=num_tokens, output_dim=dim, input_length=maxLen,\n",
        "                                weights=[embedding_matrix], trainable=True)\n",
        "    # Final matrix with pretrained embeddings that we can feed to embedding layer\n",
        "    return embedding_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uZKZm2gIubYx"
      },
      "outputs": [],
      "source": [
        "def lstm_model(input_shape, embedding_layer):\n",
        "    adam = Adam(learning_rate=5e-5)\n",
        "\n",
        "    X_indices = Input(input_shape)\n",
        "\n",
        "    x = embedding_layer(X_indices)\n",
        "    x = Bidirectional(LSTM(200, return_sequences=False)) (x)\n",
        "    x = Dense(1, activation = \"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=X_indices, outputs=x)\n",
        "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "voUyiwFNugk5"
      },
      "outputs": [],
      "source": [
        "def train_model(model, X_train_indices, Y_train_bin, X_dev_indices, Y_dev_bin, epoch_size,\n",
        "                          batch_size, encoder, output_file):\n",
        "    verbose = 1\n",
        "    batch_size = batch_size\n",
        "    epochs = epoch_size\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', restore_best_weights='True', patience=6)\n",
        "\n",
        "    model.fit(X_train_indices, Y_train_bin,\n",
        "              verbose=verbose,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              callbacks=[callback],\n",
        "              validation_data=(X_dev_indices, Y_dev_bin))\n",
        "\n",
        "       \n",
        "    test_set_predict(model, X_dev_indices, Y_dev_bin, \"val\", encoder, output_file)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "oCj9y49luixI"
      },
      "outputs": [],
      "source": [
        "def test_set_predict(model, X_dev_indices, Y_dev_bin, ident , encoder, output_file):\n",
        "    '''Do predictions and measure accuracy on our own test set (that we split off train)'''\n",
        "    # Get predictions using the trained model\n",
        "    Y_pred = model.predict(X_dev_indices)\n",
        "    # Finally, convert to numerical labels to get scores with sklearn\n",
        "    Y_pred = Y_pred>0.5\n",
        "  \n",
        "    if output_file:\n",
        "      pd.DataFrame(Y_pred).to_csv('/content/gdrive/MyDrive/Data/output_lstmdev.csv')\n",
        "    print(classification_report(Y_dev_bin, Y_pred,target_names= [\"OFF\",'NOT']))\n",
        "\n",
        "    print('Accuracy on own {1} set: {0}'.format(round(accuracy_score(Y_dev_bin, Y_pred), 3), ident))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "1GGqpM7UuxvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6c57b8-e200-4213-c016-8b74024eda0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1193514 word vectors.\n",
            "27/27 [==============================] - 1s 9ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         OFF       0.72      1.00      0.84       620\n",
            "         NOT       0.00      0.00      0.00       239\n",
            "\n",
            "    accuracy                           0.72       859\n",
            "   macro avg       0.36      0.50      0.42       859\n",
            "weighted avg       0.52      0.72      0.61       859\n",
            "\n",
            "Accuracy on own test set: 0.722\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Read in the data and embeddings\n",
        "    \n",
        "    train = pd.read_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_train.csv')\n",
        "    val = pd.read_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_val.csv')\n",
        "    test = pd.read_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_test.csv')\n",
        "    embeddings = read_embeddings('/content/gdrive/MyDrive/Data/glove.twitter.27B.100d.txt')\n",
        "\n",
        "\n",
        "    X_test, Y_test = test['preprocessed'], test['task']\n",
        "    X_train, Y_train = train['preprocessed'], train['task']\n",
        "    X_dev, Y_dev = val['preprocessed'], val['task']\n",
        "  \n",
        "  #declare and initialise values\n",
        "    tokenizer=Tokenizer()\n",
        "    epoch_size = 20\n",
        "    batch_size = 32\n",
        "    maxLen = 100\n",
        "    embedding_dim = 100\n",
        "    lstm_pretrained = True\n",
        "    output_file = False\n",
        "    val_set= False\n",
        "    test_file = True\n",
        "    \n",
        "   #converting to index and create embedding matrix\n",
        "    voc = tokenizer.word_index\n",
        "    embedding_layer = get_emb_matrix(embeddings,voc, maxLen,embedding_dim)\n",
        "\n",
        "    #Convert classes to one-hot encoding\n",
        "    encoder = LabelBinarizer()\n",
        "    encoder = encoder.fit(Y_train.tolist())\n",
        "    Y_train_bin = encoder.transform(Y_train.tolist())\n",
        "    Y_dev_bin = encoder.transform(Y_dev.tolist())\n",
        "    Y_test_bin = encoder.fit_transform(Y_test)\n",
        "   \n",
        "\n",
        "    #convert the X_train, X_dev and X_test to sequences\n",
        "    X_train_indices = tokenizer.texts_to_sequences(X_train)\n",
        "    X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')\n",
        "   \n",
        "    X_dev_indices = tokenizer.texts_to_sequences(X_dev)\n",
        "    X_dev_indices = pad_sequences(X_dev_indices, maxlen=maxLen, padding='post')\n",
        "\n",
        "    X_test_indices = tokenizer.texts_to_sequences(X_test)\n",
        "    X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n",
        "\n",
        "\n",
        "    filename = \"/content/gdrive/MyDrive/Data/lstm\"\n",
        "    if lstm_pretrained:\n",
        "      model = tf.keras.models.load_model(filename)\n",
        "      if val_set:\n",
        "        test_set_predict(model, X_dev_indices, Y_dev_bin, \"val\", encoder, output_file)\n",
        "      else:\n",
        "        test_set_predict(model, X_test_indices, Y_test_bin, \"test\", encoder, output_file)\n",
        "    else:\n",
        "      model = lstm_model(maxLen, embedding_layer)\n",
        "      model = train_model(model, X_train_indices, Y_train_bin, X_dev_indices, Y_dev_bin, epoch_size,\n",
        "                          batch_size, encoder, output_file)\n",
        "      model.save(filename)\n",
        "\n",
        "    \n",
        "      if test_file:\n",
        "        test_set_predict(model, X_test_indices, Y_test_bin, \"test\",encoder,output_file)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}