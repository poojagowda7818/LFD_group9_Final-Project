{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm75QTbTvkxP",
        "outputId": "ca483afe-03eb-4119-d19a-97c2047265ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAUVnQ6K6n9H",
        "outputId": "2df28fdd-7e4a-4513-8a73-134a83f51c1a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ZkK9yH9wtsBx"
      },
      "outputs": [],
      "source": [
        "import random as python_random\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import warnings\n",
        "from tensorflow import keras\n",
        "from keras.layers import LSTM, Activation, Dropout, Dense, Input, CuDNNLSTM\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import transformers\n",
        "from transformers import (OpenAIGPTTokenizer, TFOpenAIGPTForSequenceClassification, MobileBertTokenizer,\n",
        "                          TFMobileBertForSequenceClassification, TFAutoModelForSequenceClassification,\n",
        "                          AutoTokenizer, BertTokenizerFast, TFBertForSequenceClassification,\n",
        "                          DistilBertTokenizer, TFDistilBertForSequenceClassification,\n",
        "                          RobertaTokenizer, TFRobertaForSequenceClassification,\n",
        "                          XLNetTokenizer, TFXLNetForSequenceClassification)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "G-3uD3oAuKq9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train_model(model, tokens_train, Y_train_bin, tokens_dev, Y_dev_bin,encoder,output_file):\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "    model.fit(tokens_train, Y_train_bin, verbose=1, epochs=10,batch_size=64,callbacks=[callback], validation_data=(tokens_dev, Y_dev_bin))\n",
        "    test_set_predict(model, tokens_dev, Y_dev_bin, \"dev\",encoder,output_file)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "oCj9y49luixI"
      },
      "outputs": [],
      "source": [
        "def test_set_predict(model,tokens_dev, Y_dev_bin, ident,encoder,output_file):\n",
        "    '''Do predictions and measure accuracy on our own test set (that we split off train)'''\n",
        "    # Get predictions using the trained model\n",
        "    Y_pred = model.predict(tokens_dev)[\"logits\"]\n",
        "    Y_pred = Y_pred>0.5\n",
        "    if output_file:\n",
        "      pd.DataFrame(Y_pred).to_csv('/content/gdrive/MyDrive/Data/outputtest_dbert.csv')\n",
        "    \n",
        "    print(classification_report(Y_dev_bin, Y_pred,target_names= [\"OFF\",'NOT']))\n",
        "    print('Accuracy on own {1} set: {0}'.format(round(accuracy_score(Y_dev_bin, Y_pred), 3), ident))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "1GGqpM7UuxvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbcc26cc-7711-4d33-b66f-4b6c5314bfd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_897']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "125/125 [==============================] - 86s 614ms/step - loss: 0.5386 - accuracy: 0.7000 - val_loss: 0.5141 - val_accuracy: 0.7918\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 75s 599ms/step - loss: 0.4018 - accuracy: 0.8184 - val_loss: 0.4674 - val_accuracy: 0.7978\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 75s 599ms/step - loss: 0.2761 - accuracy: 0.8830 - val_loss: 0.5521 - val_accuracy: 0.7718\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 75s 598ms/step - loss: 0.1573 - accuracy: 0.9438 - val_loss: 0.7762 - val_accuracy: 0.7618\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 75s 598ms/step - loss: 0.1036 - accuracy: 0.9655 - val_loss: 0.8200 - val_accuracy: 0.7758\n",
            "32/32 [==============================] - 5s 103ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         OFF       0.84      0.81      0.82       647\n",
            "         NOT       0.67      0.71      0.69       352\n",
            "\n",
            "    accuracy                           0.78       999\n",
            "   macro avg       0.75      0.76      0.76       999\n",
            "weighted avg       0.78      0.78      0.78       999\n",
            "\n",
            "Accuracy on own dev set: 0.776\n",
            "27/27 [==============================] - 4s 102ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         OFF       0.86      0.87      0.87       620\n",
            "         NOT       0.66      0.64      0.65       239\n",
            "\n",
            "    accuracy                           0.81       859\n",
            "   macro avg       0.76      0.76      0.76       859\n",
            "weighted avg       0.81      0.81      0.81       859\n",
            "\n",
            "Accuracy on own test set: 0.809\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  train = pd.read_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_train.csv')\n",
        "  val= pd.read_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_val.csv')\n",
        "  test = pd.read_csv('/content/gdrive/MyDrive/Data/preprocessed data/processed_test.csv')\n",
        "  X_train, Y_train = train['tweet'], train['task']\n",
        "  X_dev, Y_dev = val['tweet'], val['task']\n",
        "  X_test, Y_test = test['tweet'],test['task']\n",
        "\n",
        "  encoder = LabelBinarizer()\n",
        "  encoder = encoder.fit(Y_train.tolist())\n",
        "  Y_train_bin = encoder.transform(Y_train.tolist())\n",
        "  # Use encoder.classes_ to find mapping back\n",
        "  Y_dev_bin = encoder.transform(Y_dev.tolist())      \n",
        "  Y_test_bin = encoder.fit_transform(Y_test)\n",
        "  filename = \"/content/gdrive/MyDrive/Data/dbert\"\n",
        "  bert_pretrained= False\n",
        "  custom_test_set = False\n",
        "  output_file = False\n",
        "  val_set = False\n",
        "  if bert_pretrained:\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(filename, local_files_only=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    tokenizer.pad_token = \"[PAD]\"\n",
        "    tokens_test = tokenizer(X_test.values.tolist(), padding=True, max_length=100, truncation=True,\n",
        "                           return_tensors=\"np\").data\n",
        "    test_set_predict(model, tokens_test, Y_test_bin, \"test\",encoder,output_file)\n",
        "  else:\n",
        "    # lm = \"bert-base-uncased\"\n",
        "    # lm= \"roberta-base\"\n",
        "    lm = \"distilbert-base-uncased\"\n",
        "    optim = Adam(learning_rate=5e-5)\n",
        "    loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True) \n",
        "    tokenizer = AutoTokenizer.from_pretrained(lm)\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(lm, num_labels=1)\n",
        "    tokens_train = tokenizer(X_train.tolist(), padding=True, max_length=100,truncation=True, return_tensors=\"np\").data\n",
        "    tokens_dev = tokenizer(X_dev.tolist(), padding=True, max_length=100,truncation=True, return_tensors=\"np\").data\n",
        "    model.compile(loss=loss_function, optimizer=optim, metrics=['accuracy'])\n",
        "    model = train_model(model, tokens_train, Y_train_bin, tokens_dev, Y_dev_bin,output_file,encoder)\n",
        "    test_file = True\n",
        "    if test_file:\n",
        "      tokens_test = tokenizer(X_test.tolist(), padding=True, max_length=100,truncation=True, return_tensors=\"np\").data\n",
        "      test_set_predict(model, tokens_test, Y_test_bin, \"test\",encoder,output_file)\n",
        "    model.save_pretrained(filename)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}